# Gradient-Descent-Implementation-With-Ridge-Regression


Overview
This project implements a Ridge Regression model using Gradient Descent (GDRegularisedLinearRegressor) to minimize the regularized least squares loss function. 
The model is evaluated on a synthetic dataset generated using a custom function, and the performance is measured across various values of the regularization parameter λ (lambda).

The project also demonstrates the use of a custom polynomial feature transformer and a pipeline to preprocess the data and fit the model. 
The final output is a plot comparing the training and testing errors across different λ values, plotted on a logarithmic scale.

Key Features

GDRegularisedLinearRegressor: A Ridge Regression model that uses gradient descent for optimization.
PolynomialFeatures Transformer: A custom polynomial feature transformer to allow the model to capture non-linear relationships in the data.
Cross-Validation: The model is fit multiple times across different λ values to find the best regularization parameter that minimizes test error.
Noise-Added Dataset: A custom function generates noisy, additive data, providing a challenging dataset for the regressor.
Error Comparison: The project visualizes training and testing errors across a wide range of λ values to understand the effect of regularization.

Data Generation
The dataset is generated by a custom function that returns noisy observations based on a known function f(x) - a cosine function

Model Fitting
We fit the GDRegularisedLinearRegressor across different λ values using the PolynomialFeatures to create polynomial terms for the input data

Model Evaluation
Training and test errors are calculated and plotted for different λ values
Finally, we plot the training and testing errors on a logarithmic scale to compare model performance

